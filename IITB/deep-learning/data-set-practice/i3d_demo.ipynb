{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I3D (Inflated 3D ConvNet) for Video Classification\n",
    "\n",
    "This notebook demonstrates how to use a pre-trained I3D model from TensorFlow Hub for video action recognition. I3D models are a type of 3D Convolutional Neural Network that are 'inflated' from 2D image classification architectures (like Inception-v1), making them effective for learning spatio-temporal features from video data.\n",
    "\n",
    "We will:\n",
    "1.  Load a pre-trained I3D model (trained on the Kinetics-400 dataset).\n",
    "2.  Download a sample video.\n",
    "3.  Preprocess the video to fit the model's input requirements.\n",
    "4.  Run inference to classify the action in the video.\n",
    "5.  Display the top predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary libraries. If you are running this in a Google Colab environment, this step is straightforward. If you are running locally, make sure you have a compatible Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages. Uncomment the line below if you don't have them installed.\n",
    "!pip install -q tensorflow tensorflow-hub opencv-python-headless imageio matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import all the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from urllib import request\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Labels\n",
    "\n",
    "We'll load the I3D model pre-trained on the Kinetics-400 dataset from TensorFlow Hub. We also need the corresponding class labels to interpret the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the I3D model on TensorFlow Hub\n",
    "I3D_MODEL_URL = \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"\n",
    "\n",
    "# URL for the Kinetics-400 labels file\n",
    "KINETICS_400_LABELS_URL = 'https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt'\n",
    "KINETICS_400_LABELS_PATH = 'kinetics_400_labels.txt'\n",
    "\n",
    "# Download the labels file\n",
    "if not os.path.exists(KINETICS_400_LABELS_PATH):\n",
    "    print(f\"Downloading {KINETICS_400_LABELS_URL} to {KINETICS_400_LABELS_PATH}\")\n",
    "    request.urlretrieve(KINETICS_400_LABELS_URL, KINETICS_400_LABELS_PATH)\n",
    "\n",
    "# Load the labels\n",
    "with open(KINETICS_400_LABELS_PATH) as f:\n",
    "    kinetics_400_labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Loaded {len(kinetics_400_labels)} labels from Kinetics-400 dataset.\")\n",
    "print(\"Example labels:\", kinetics_400_labels[:5])\n",
    "\n",
    "# Load the I3D model\n",
    "print(\"\\nLoading I3D model from TensorFlow Hub...\")\n",
    "i3d = hub.load(I3D_MODEL_URL)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Helper Function\n",
    "\n",
    "The I3D model expects a video tensor of a specific shape and format. The input tensor should be `[batch_size, num_frames, height, width, 3]`, where the pixel values are normalized to be between 0 and 1. We'll create a helper function to load a video from a path (or URL) and preprocess it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, max_frames=64, resize=(224, 224)):\n",
    "    \"\"\"Loads a video from a path, normalizes it, and returns a tensor.\"\"\"\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # OpenCV reads in BGR, convert to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Resize frame\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            # Normalize to [0, 1]\n",
    "            frame = frame / 255.0\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    if len(frames) < 1:\n",
    "        return np.array([])\n",
    "        \n",
    "    # Add a batch dimension\n",
    "    video_tensor = np.expand_dims(frames, axis=0)\n",
    "    return video_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Sample Video and Run Inference\n",
    "\n",
    "Let's test our setup with a sample video. We'll download a GIF of someone playing guitar, which corresponds to the 'playing guitar' class in the Kinetics-400 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample video URL (e.g., a person playing guitar)\n",
    "SAMPLE_VIDEO_URL = \"https://upload.wikimedia.org/wikipedia/commons/8/86/End_of_a_jam.gif\"\n",
    "SAMPLE_VIDEO_PATH = \"sample.gif\"\n",
    "\n",
    "# Download the sample video\n",
    "if not os.path.exists(SAMPLE_VIDEO_PATH):\n",
    "    print(f\"Downloading {SAMPLE_VIDEO_URL} to {SAMPLE_VIDEO_PATH}\")\n",
    "    request.urlretrieve(SAMPLE_VIDEO_URL, SAMPLE_VIDEO_PATH)\n",
    "\n",
    "# Display the downloaded video\n",
    "print(\"\\nSample video:\")\n",
    "display(Image(SAMPLE_VIDEO_PATH, width=300))\n",
    "\n",
    "# Load and preprocess the video\n",
    "print(\"\\nPreprocessing video...\")\n",
    "video_tensor = load_video(SAMPLE_VIDEO_PATH)\n",
    "print(f\"Video tensor shape: {video_tensor.shape}\")\n",
    "\n",
    "# Ensure the video tensor is not empty\n",
    "if video_tensor.size == 0:\n",
    "    print(\"Could not load video. Please check the path or URL.\")\n",
    "else:\n",
    "    # The model expects a float32 tensor\n",
    "    video_tensor = tf.convert_to_tensor(video_tensor, dtype=tf.float32)\n",
    "\n",
    "    # Get the model's signature\n",
    "    model_signature = i3d.signatures['default']\n",
    "\n",
    "    # Run inference\n",
    "    print(\"\\nRunning inference...\")\n",
    "    logits = model_signature(video_tensor)['default']\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "    # Get the top 5 predictions\n",
    "    top_k = 5\n",
    "    top_predictions = tf.math.top_k(probabilities, k=top_k)\n",
    "    top_indices = top_predictions.indices.numpy().flatten()\n",
    "    top_probs = top_predictions.values.numpy().flatten()\n",
    "\n",
    "    print(f\"\\nTop {top_k} predictions:\")\n",
    "    for i in range(top_k):\n",
    "        label = kinetics_400_labels[top_indices[i]]\n",
    "        prob = top_probs[i]\n",
    "        print(f\"{i+1}. {label}: {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try Another Example\n",
    "\n",
    "Let's try another video, this time of someone doing a front flip. This corresponds to the 'front flip' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another sample video URL (e.g., a person doing a front flip)\n",
    "SAMPLE_VIDEO_URL_2 = \"https://upload.wikimedia.org/wikipedia/commons/4/42/Front_flip.gif\"\n",
    "SAMPLE_VIDEO_PATH_2 = \"sample_2.gif\"\n",
    "\n",
    "# Download the sample video\n",
    "if not os.path.exists(SAMPLE_VIDEO_PATH_2):\n",
    "    print(f\"Downloading {SAMPLE_VIDEO_URL_2} to {SAMPLE_VIDEO_PATH_2}\")\n",
    "    request.urlretrieve(SAMPLE_VIDEO_URL_2, SAMPLE_VIDEO_PATH_2)\n",
    "\n",
    "# Display the downloaded video\n",
    "print(\"\\nSample video 2:\")\n",
    "display(Image(SAMPLE_VIDEO_PATH_2, width=300))\n",
    "\n",
    "# Load and preprocess the video\n",
    "print(\"\\nPreprocessing video...\")\n",
    "video_tensor_2 = load_video(SAMPLE_VIDEO_PATH_2)\n",
    "print(f\"Video tensor shape: {video_tensor_2.shape}\")\n",
    "\n",
    "if video_tensor_2.size == 0:\n",
    "    print(\"Could not load video. Please check the path or URL.\")\n",
    "else:\n",
    "    video_tensor_2 = tf.convert_to_tensor(video_tensor_2, dtype=tf.float32)\n",
    "\n",
    "    # Run inference\n",
    "    print(\"\\nRunning inference...\")\n",
    "    logits_2 = model_signature(video_tensor_2)['default']\n",
    "    probabilities_2 = tf.nn.softmax(logits_2)\n",
    "\n",
    "    # Get the top 5 predictions\n",
    "    top_k = 5\n",
    "    top_predictions_2 = tf.math.top_k(probabilities_2, k=top_k)\n",
    "    top_indices_2 = top_predictions_2.indices.numpy().flatten()\n",
    "    top_probs_2 = top_predictions_2.values.numpy().flatten()\n",
    "\n",
    "    print(f\"\\nTop {top_k} predictions:\")\n",
    "    for i in range(top_k):\n",
    "        label = kinetics_400_labels[top_indices_2[i]]\n",
    "        prob = top_probs_2[i]\n",
    "        print(f\"{i+1}. {label}: {prob:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
